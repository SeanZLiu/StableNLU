
5/11：
    ·需要对比一下train_roberta.py和train_bert_features.py，在main model的训练中引入
    (此前修改已commit至 new branch second commit)
    ·后续可以把bert引用的库从pytorch_pretrained_bert更新为transformer，但该工作似乎优先
    级较低
    ·注意func目录下的各函数来自此前train的非main部分，分别用于加载数据、转换为feature、定
    义超参数

5/13:
    ·新分支第三次commit："newbran 3rd commit, add shallow feature to all class in
    bert_distill.py, default None, add attribute shallow feature to
    class InputFeatures default None, and ExampleConverter,
    copy handle_shallow_features method from train_roberta_distill.py"
    ·后续需要在训练时（如果为main model的训练）调用handle_shallow_features方法读取shallow
    feature文件，eval时不要调用，并for循环为每个样本设置shallow_feature属性，随后需要在collate方法中设
    置输出列表，对应train中不同情况（有shallow_feature和无sf）和eval需要的格式，对应的不
    同情况也需要在命令行参数中设置对应选项
    ·为了完成上面这一点，设置命令行参数train_with_feature，true的时候会添加shallow feature
    ·应注意的是，修改后的bert_distill.py对应的forward方法，在训练中如果不传入shallow_feature，
    则参数默认为None并且不会添加到线性层的输入中，若传入参数则会相加

5/14:
    ·新分支第四次commit："newbran 4th commit, add new args train_with_features, set true
    if need to train model with shallow features, add for loop to set shallow_feature
    attribute for each example on this situation, changed collate method to output different
    type of list based on whether this attribute is None, and changed model input when training."

5/15:
    ·在服务器调整修改了关于Roberta和ERNIE的训练文并进行训练，服务器端commit，已同步本地git仓库

5/16:
    ·分别得到RoBERTa和ERNIE作为main model的第一组结果，切换种子继续训练得到一组平均结果，注意学习率均为默认学习率，需要调整
    ·后面分别训练一组shallow model，对比一下由bert和另外两个生成的shallow model抽取到的shallow feature对main model的训练效果有什么提升

5/17:
    ·已经用1e-5学习率训练了一组roberta的shallow model，开始用种子791训练main model，后续对比和bert feature的效果区别
    ·ernie部分对应的结果并没有提高，可能是ernie main的学习率的问题
    ·roberta获得了两组结果，ernie用3e-5得到了两组结果
